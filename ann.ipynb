{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Tracking an Artificial Neural Network on the MNIST Dataset Using PyTorch and MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Summary\n",
    "\n",
    "This Python script builds, trains, and evaluates an Artificial Neural Network (ANN) model on the MNIST dataset using PyTorch, and logs the entire experiment with MLflow.\n",
    "\n",
    "Key steps:\n",
    "\n",
    "Hyperparameters: Sets learning rate, batch size, and number of epochs.\n",
    "\n",
    "Data Preprocessing: Loads the MNIST dataset and applies normalization using torchvision.transforms.\n",
    "\n",
    "Model Definition: Defines a simple feedforward ANN with three fully connected layers:\n",
    "\n",
    "Input Layer: 28x28 pixels (784) flattened into a vector.\n",
    "\n",
    "Two hidden layers with 128 and 64 units, using ReLU activation.\n",
    "\n",
    "Output Layer: 10 units for classifying digits (0-9).\n",
    "\n",
    "Training:\n",
    "The train() function iterates over the training data, computes the loss using cross-entropy, and updates the model's weights using Adam optimizer.\n",
    "\n",
    "Accuracy and loss are logged after each epoch.\n",
    "\n",
    "Evaluation: The test() function computes the accuracy and loss on the test dataset.\n",
    "MLflow Integration:\n",
    "Parameters (learning rate, batch size, epochs) and metrics (loss, accuracy) are logged during training and testing for each epoch.\n",
    "\n",
    "The final trained model is saved using mlflow.pytorch.log_model().\n",
    "After training, the model and results are tracked in MLflow for reproducibility and analysis.\n",
    "\n",
    "This workflow is useful for experimenting with hyperparameters, tracking performance, and storing models efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:00<00:00, 11.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 81.5kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:07<00:00, 228kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "           )"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)  # Flatten the input\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        correct += (predicted == target).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    print(f'Train Epoch: {epoch} Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            total_loss += criterion(output, target).item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f'Test Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ANN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 Loss: 0.2738, Accuracy: 91.82%\n",
      "Test Loss: 0.1309, Accuracy: 95.94%\n",
      "Train Epoch: 2 Loss: 0.1151, Accuracy: 96.42%\n",
      "Test Loss: 0.0979, Accuracy: 96.88%\n",
      "Train Epoch: 3 Loss: 0.0802, Accuracy: 97.47%\n",
      "Test Loss: 0.0929, Accuracy: 96.98%\n",
      "Train Epoch: 4 Loss: 0.0618, Accuracy: 98.01%\n",
      "Test Loss: 0.0846, Accuracy: 97.37%\n",
      "Train Epoch: 5 Loss: 0.0508, Accuracy: 98.34%\n",
      "Test Loss: 0.0894, Accuracy: 97.34%\n",
      "Train Epoch: 6 Loss: 0.0395, Accuracy: 98.71%\n",
      "Test Loss: 0.0780, Accuracy: 97.69%\n",
      "Train Epoch: 7 Loss: 0.0355, Accuracy: 98.81%\n",
      "Test Loss: 0.0875, Accuracy: 97.52%\n",
      "Train Epoch: 8 Loss: 0.0287, Accuracy: 98.99%\n",
      "Test Loss: 0.0848, Accuracy: 97.74%\n",
      "Train Epoch: 9 Loss: 0.0242, Accuracy: 99.17%\n",
      "Test Loss: 0.0984, Accuracy: 97.43%\n",
      "Train Epoch: 10 Loss: 0.0264, Accuracy: 99.12%\n",
      "Test Loss: 0.0872, Accuracy: 97.83%\n",
      "Training and tracking with MLflow completed.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"epochs\", epochs)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_acc = train(model, device, train_loader, optimizer, criterion, epoch)\n",
    "        test_loss, test_acc = test(model, device, test_loader, criterion)\n",
    "        \n",
    "        # Log metrics to MLflow\n",
    "        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "        mlflow.log_metric(\"train_accuracy\", train_acc, step=epoch)\n",
    "        mlflow.log_metric(\"test_loss\", test_loss, step=epoch)\n",
    "        mlflow.log_metric(\"test_accuracy\", test_acc, step=epoch)\n",
    "\n",
    "\n",
    "    mlflow.pytorch.log_model(model, \"model\")\n",
    "\n",
    "print(\"Training and tracking with MLflow completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Artificial Neural Network (ANN) model, trained on the MNIST dataset using PyTorch and tracked via MLflow, achieved impressive results. After 10 epochs, the model reached a Test Loss of 0.0872 and a Test Accuracy of 97.83%, demonstrating strong performance in digit classification. The use of MLflow allowed for efficient tracking of hyperparameters, training metrics, and the final model, facilitating future experimentation and model optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
